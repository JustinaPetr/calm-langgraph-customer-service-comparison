from custom_test_runner import TOTAL_COST, COMPLETION_TOKENS, PROMPT_TOKENS
from rasa.dialogue_understanding.generator import LLMCommandGenerator

from typing import Any, List, Optional, Text

from langchain.callbacks import OpenAICallbackHandler

import structlog
from rasa.dialogue_understanding.commands import (
    Command
)
from rasa.engine.recipes.default_recipe import DefaultV1Recipe
from rasa.shared.exceptions import ProviderClientAPIException
from rasa.shared.core.flows import FlowsList
from rasa.shared.core.trackers import DialogueStateTracker

from rasa.shared.nlu.training_data.message import Message
from rasa.shared.utils.llm import (
    DEFAULT_OPENAI_CHAT_MODEL_NAME_ADVANCED,
    DEFAULT_OPENAI_MAX_GENERATED_TOKENS,
    llm_factory,
)
COMMAND_PROMPT_FILE_NAME = "command_prompt.jinja2"
DEFAULT_LLM_CONFIG = {
    "_type": "openai",
    "request_timeout": 7,
    "temperature": 0.0,
    "model_name": DEFAULT_OPENAI_CHAT_MODEL_NAME_ADVANCED,
    "max_tokens": DEFAULT_OPENAI_MAX_GENERATED_TOKENS,
}
LLM_CONFIG_KEY = "llm"
USER_INPUT_CONFIG_KEY = "user_input"
FLOW_RETRIEVAL_KEY = "flow_retrieval"
FLOW_RETRIEVAL_ACTIVE_KEY = "active"
structlogger = structlog.get_logger()


@DefaultV1Recipe.register(
    [
        DefaultV1Recipe.ComponentType.COMMAND_GENERATOR,
    ],
    is_trainable=True,
)
class CustomLLMCommandGenerator(LLMCommandGenerator):

    async def predict_commands(
        self,
        message: Message,
        flows: FlowsList,
        tracker: Optional[DialogueStateTracker] = None,
        **kwargs: Any,
    ) -> List[Command]:
        """Predict commands using the LLM.

        Args:
            message: The message from the user.
            flows: The flows available to the user.
            tracker: The tracker containing the current state of the conversation.

        Returns:
            The commands generated by the llm.
        """
        self.openai_callback_handler = OpenAICallbackHandler()

        commands = await super(CustomLLMCommandGenerator, self).predict_commands(message, flows, tracker)

        message.set(
            COMPLETION_TOKENS,
            self.openai_callback_handler.completion_tokens,
            add_to_output=True,
        )
        message.set(
            PROMPT_TOKENS,
            self.openai_callback_handler.prompt_tokens,
            add_to_output=True,
        )
        message.set(
            TOTAL_COST,
            self.openai_callback_handler.total_cost,
            add_to_output=True,
        )

        return commands

    async def invoke_llm(self, prompt: Text) -> Optional[Text]:
        """Use LLM to generate a response.

        Args:
            prompt: The prompt to send to the LLM.

        Returns:
            The generated text.

        Raises:
            ProviderClientAPIException if an error during API call.
        """
        llm = llm_factory(self.config.get(LLM_CONFIG_KEY), DEFAULT_LLM_CONFIG)
        try:
            return await llm.apredict(prompt, callbacks=[self.openai_callback_handler])
        except Exception as e:
            # unfortunately, langchain does not wrap LLM exceptions which means
            # we have to catch all exceptions here
            structlogger.error("llm_based_command_generator.llm.error", error=e)
            raise ProviderClientAPIException(
                message="LLM call exception", original_exception=e
            )
